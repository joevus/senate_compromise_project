---
title: "senate_compromise_words"
author: "hoskisson"
date: "2023-02-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries
```{r}
library(quanteda) # https://quanteda.io/
library("quanteda.textplots")
library("quanteda.textstats")
library(tidyverse)
```




*Compromise*
Respect for Marriage Act of 2022
Bipartisan Safer Communities Act of 2022
Infrastructure Investment and Jobs Act of 2021

*Non Compromise*
Inflation Reduction Act of 2022
American Rescue Plan of 2021


# Data prep

Read Data
```{r}
# Use RDS file because CSV file lost data after it turned some digits in the id and id_str strings
# to zeros.
american_rescue <- readRDS("data/raw_tweets/american_rescue_2021_corrected.rds") %>%
  # this one doesn't have full_text or retweet_count
  select(c(created_at, id, id_str, text, username, author_id)) %>%
  rename (
    user_id = author_id,
    screen_name = username
  ) %>%
  mutate(
    full_text = NA,
    user_id_str = NA,
    followers_count = NA,
    retweet_count = NA
  )

inflation_reduction <- read.csv("data/raw_tweets/inflation_reduction.csv") %>%
  select(c(created_at, id, id_str, text, full_text, retweet_count))
inflation_reduction_users <- read.csv("data/raw_tweets/inflation_reduction_users.csv") %>%
  rename(
    user_id = id,
    user_id_str = id_str,
  ) %>%
  select(c(user_id, user_id_str, screen_name, followers_count))

infrastructure_investment <- read.csv("data/raw_tweets/infrastructure_investment.csv") %>%
  select(c(created_at, id, id_str, text, full_text, retweet_count))
infrastructure_investment_users <- read.csv("data/raw_tweets/infrastructure_users.csv") %>%
  rename(
    user_id = id,
    user_id_str = id_str,
  ) %>%
  select(c(user_id, user_id_str, screen_name, followers_count))

respect_for_marriage <- read.csv("data/raw_tweets/respect_for_marriage.csv") %>%
  select(c(created_at, id, id_str, text, full_text, retweet_count))
respect_for_marriage_users <- read.csv("data/raw_tweets/respect_for_marriage_users.csv") %>%
  rename(
    user_id = id,
    user_id_str = id_str,
  ) %>%
  select(c(user_id, user_id_str, screen_name, followers_count))

safer_communities <- read.csv("data/raw_tweets/safer_communities.csv") %>%
  select(c(created_at, id, id_str, text, full_text, retweet_count))
safer_communities_users <- read.csv("data/raw_tweets/safer_communities_users.csv") %>%
  rename(
    user_id = id,
    user_id_str = id_str,
  ) %>%
  select(c(user_id, user_id_str, screen_name, followers_count))
```

Combine Tweet and user data
```{r}
# american rescue collection already has user data included

inflation_reduction <- inflation_reduction %>%
  cbind(inflation_reduction_users)

infrastructure_investment <- infrastructure_investment %>%
  cbind(infrastructure_investment_users)

respect_for_marriage <- respect_for_marriage %>%
  cbind(respect_for_marriage_users)

safer_communities <- safer_communities %>%
  cbind(safer_communities_users)
```

Narrow dates to day before bill passed
```{r}
# american rescue data already narrowed
# however, may only go to 3/4/2021 instead of 3/5/2021

inflation_reduction <- inflation_reduction %>% filter(created_at < '2022-08-07')

infrastructure_investment <- infrastructure_investment %>% filter(created_at < '2021-08-10')

respect_for_marriage <- respect_for_marriage %>% filter(created_at < '2022-11-29')

safer_communities <- safer_communities %>% filter(created_at < '2022-06-23')
```


## Enhance data

Read in covariates and senator list
```{r}
senator_variables <- read.csv("data/senator_data_d.csv")
senator_handles <- read.csv("data/senator_handles_table.csv")
```

Join senator handles to tweet collections to get senator ids in tweet collections
Then, combine covariates with tweet collections by joining by senator id
```{r}
american_rescue$screen_name <- tolower(american_rescue$screen_name)
inflation_reduction$screen_name <- tolower(inflation_reduction$screen_name)
infrastructure_investment$screen_name <- tolower(infrastructure_investment$screen_name)
respect_for_marriage$screen_name <- tolower(respect_for_marriage$screen_name)
safer_communities$screen_name <- tolower(safer_communities$screen_name)

american_rescue <- american_rescue %>%
  left_join(select(senator_handles, senator_handles_list, senator_id), by = c("screen_name" = "senator_handles_list")) %>%
  left_join(select(senator_variables, -c(type, full_name)), by = c("senator_id" = "id"))

inflation_reduction <- inflation_reduction %>%
  left_join(select(senator_handles, senator_handles_list, senator_id), by = c("screen_name" = "senator_handles_list")) %>%
  left_join(select(senator_variables, -c(type, full_name)), by = c("senator_id" = "id"))

infrastructure_investment <- infrastructure_investment %>%
  left_join(select(senator_handles, senator_handles_list, senator_id), by = c("screen_name" = "senator_handles_list")) %>%
  left_join(select(senator_variables, -c(type, full_name)), by = c("senator_id" = "id"))

respect_for_marriage <- respect_for_marriage %>%
  left_join(select(senator_handles, senator_handles_list, senator_id), by = c("screen_name" = "senator_handles_list")) %>%
  left_join(select(senator_variables, -c(type, full_name)), by = c("senator_id" = "id"))

safer_communities <- safer_communities %>%
  left_join(select(senator_handles, senator_handles_list, senator_id), by = c("screen_name" = "senator_handles_list")) %>%
  left_join(select(senator_variables, -c(type, full_name)), by = c("senator_id" = "id"))
```

Add age
```{r}
american_rescue <- american_rescue %>%
  mutate(created_at = as.Date(created_at)) %>%
  mutate(birthday = as.Date(birthday)) %>%
  mutate(age = as.numeric(difftime(created_at, birthday, units = "days"))/365) %>%
  mutate(age = floor(age))

inflation_reduction <- inflation_reduction %>%
  mutate(created_at = as.Date(created_at)) %>%
  mutate(birthday = as.Date(birthday)) %>%
  mutate(age = as.numeric(difftime(created_at, birthday, units = "days"))/365) %>%
  mutate(age = floor(age))

infrastructure_investment <- infrastructure_investment %>%
  mutate(created_at = as.Date(created_at)) %>%
  mutate(birthday = as.Date(birthday)) %>%
  mutate(age = as.numeric(difftime(created_at, birthday, units = "days"))/365) %>%
  mutate(age = floor(age))

respect_for_marriage <- respect_for_marriage %>%
  mutate(created_at = as.Date(created_at)) %>%
  mutate(birthday = as.Date(birthday)) %>%
  mutate(age = as.numeric(difftime(created_at, birthday, units = "days"))/365) %>%
  mutate(age = floor(age))

safer_communities <- safer_communities %>%
  mutate(created_at = as.Date(created_at)) %>%
  mutate(birthday = as.Date(birthday)) %>%
  mutate(age = as.numeric(difftime(created_at, birthday, units = "days"))/365) %>%
  mutate(age = floor(age))
```



Add one column to each Tweet collection for whether the collection was connected
to a bipartisan bill or not. Uses the variable name `bipartisan_bill`. Coded
`1` for a bipartisan bill and `0` for a partisan bill.
```{r}
american_rescue$bipartisan_bill = 0
inflation_reduction$bipartisan_bill = 0

infrastructure_investment$bipartisan_bill = 1
respect_for_marriage$bipartisan_bill = 1
safer_communities$bipartisan_bill = 1
```

Add column for number of bipartisan votes for the bill associated with the Tweet collection.
One vote for each Senator from the minority party who voted for the bill. In each
collection these are Republican Senators.
For those collections coded as `0` this indicates the Tweet belongs to a partisan bill collection
of Tweets (again meaning Tweets in the 2-week period leading up to a passing vote on a bill).
```{r}
american_rescue$bipartisan_votes = 0
inflation_reduction$bipartisan_votes = 0

infrastructure_investment$bipartisan_votes = 19
respect_for_marriage$bipartisan_votes = 12
safer_communities$bipartisan_votes = 15
```



Group data
```{r}
bipartisan_collection <- rbind(respect_for_marriage, safer_communities, infrastructure_investment)
partisan_collection <- rbind(inflation_reduction, american_rescue)
all_tweets <- rbind(bipartisan_collection, partisan_collection)
```


lemma data
```{r}
lemmaData <- read.csv2("data/baseform_en.tsv", # downloaded from https://github.com/tm4ss/tm4ss.github.io/tree/master/resources
                       sep=",", 
                       header=FALSE, 
                       encoding = "UTF-8", 
                       stringsAsFactors = F)
```

corpus function
```{r}

tweets_to_corpus<-function(tweets, num_of_ngrams) {
  tweets_corpus <- corpus(tweets,
                          docid_field = "id_str",
                          text_field="text")
  
  tweets_corpus_proc <- tokens(tweets_corpus,
                                remove_punct = TRUE, # remove punctuation
                                remove_numbers= TRUE, # remove numbers
                                remove_symbols= TRUE) %>% # remove symbols
                                  tokens_tolower() # remove capitalization
  tweets_corpus_proc <- tokens_replace(
                          tweets_corpus_proc, # "Substitute token types based on vectorized one-to-one matching"
                          lemmaData$V1,
                          lemmaData$V2,
                          valuetype = "fixed"
                        )
  
  tweets_corpus_proc <- tweets_corpus_proc %>%
                          tokens_remove(stopwords("english")) %>%
                          tokens_remove(c("amp", "rt")) %>%
                          tokens_ngrams(num_of_ngrams)
 
  tweets_corpus_proc # return the processed corpus
}
```

Create corpus. These have 1 or 2 as the second parameter indicating ngrams.
A corpus with the `a` suffix indicates ngram of 1. Suffix of `b` indicates 
ngram of 2.
Also, what I call a corpus here is actually a set of tokens.
```{r}
american_rescue_corpus_a <- tweets_to_corpus(american_rescue, 1)
american_rescue_corpus_b <- tweets_to_corpus(american_rescue, 2)

inflation_reduction_corpus_a <- tweets_to_corpus(inflation_reduction, 1)
inflation_reduction_corpus_b <- tweets_to_corpus(inflation_reduction, 2)

infrastructure_investment_corpus_a <- tweets_to_corpus(infrastructure_investment, 1)
infrastructure_investment_corpus_b <- tweets_to_corpus(infrastructure_investment, 2)

respect_for_marriage_corpus_a <- tweets_to_corpus(respect_for_marriage, 1)
respect_for_marriage_corpus_b <- tweets_to_corpus(respect_for_marriage, 2)

safer_communities_corpus_a <- tweets_to_corpus(safer_communities, 1)
safer_communities_corpus_b <- tweets_to_corpus(safer_communities, 2)

bipartisan_collection_corpus_a <- tweets_to_corpus(bipartisan_collection, 1)
bipartisan_collection_corpus_b <- tweets_to_corpus(bipartisan_collection, 2)

partisan_collection_corpus_a <- tweets_to_corpus(partisan_collection, 1)
partisan_collection_corpus_b <- tweets_to_corpus(partisan_collection, 2)

all_tweets_corpus_a <- tweets_to_corpus(all_tweets, 1)
all_tweets_corpus_b <- tweets_to_corpus(all_tweets, 2)
```


dtms
```{r}
dtm_american_rescue_a<-dfm(american_rescue_corpus_a)
dtm_american_rescue_b<-dfm(american_rescue_corpus_b)

dtm_inflation_reduction_a<-dfm(inflation_reduction_corpus_a)
dtm_inflation_reduction_b<-dfm(inflation_reduction_corpus_b)

dtm_infrastructure_investment_a<-dfm(infrastructure_investment_corpus_a)
dtm_infrastructure_investment_b<-dfm(infrastructure_investment_corpus_b)

dtm_respect_for_marriage_a<-dfm(respect_for_marriage_corpus_a)
dtm_respect_for_marriage_b<-dfm(respect_for_marriage_corpus_b)

dtm_safer_communities_a<-dfm(safer_communities_corpus_a)
dtm_safer_communities_b<-dfm(safer_communities_corpus_b)

dtm_bipartisan_collection_a<-dfm(bipartisan_collection_corpus_a)
dtm_bipartisan_collection_b<-dfm(bipartisan_collection_corpus_b)

dtm_partisan_collection_a<-dfm(partisan_collection_corpus_a)
dtm_partisan_collection_b<-dfm(partisan_collection_corpus_b)

dtm_all_tweets_a<-dfm(all_tweets_corpus_a)
dtm_all_tweets_b<-dfm(all_tweets_corpus_b)
```



# Highest frequency words

Notes

To access a list of the most frequently occurring features, we can use `topfeatures():`

`topfeatures(dfmat_uk, 20)  # 20 most frequent words`

from https://quanteda.io/articles/quickstart.html


## Top words overall
```{r}
top_words_all <- topfeatures(dtm_all_tweets_a, 10)

top_words_all_df <- data.frame(count = top_words_all,
                               word = names(top_words_all))
row.names(top_words_all_df) <- NULL

p_top_words_all <- ggplot(data=top_words_all_df, aes(x=reorder(word, -count), y=count)) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="Top word count - All Tweets") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = -45, size =14),
    plot.title = element_text(hjust = 0.5)
  ) +
  ylim(0,4000)


p_top_words_all
```

Save chart
```{r}
ggsave("plots/p_top_words_all.png", plot = p_top_words_all)
```




## Top words in each act collection

```{r}
topfeatures(dtm_american_rescue_a, 20)
```

```{r}
topfeatures(dtm_inflation_reduction_a, 20)
```

```{r}
topfeatures(dtm_infrastructure_investment_a, 20)
```

```{r}
topfeatures(dtm_respect_for_marriage_a, 20)
```

```{r}
topfeatures(dtm_safer_communities_a, 20)
```


## Top words in compromise Tweets versus not compromise Tweets

### Bipartisan collection

```{r}
top_words_bipartisan <- topfeatures(dtm_bipartisan_collection_a, 10)

top_words_bipartisan_df <- data.frame(count = top_words_bipartisan,
                               word = names(top_words_bipartisan))
row.names(top_words_bipartisan_df) <- NULL

p_top_words_bipartisan_df <- ggplot(data=top_words_bipartisan_df, aes(x=reorder(word, -count), y=count)) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="Top word count, bipartisan collection") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = -45, size =14),
    plot.title = element_text(hjust = 0.5)
  ) +
  ylim(0, 2000)

p_top_words_bipartisan_df
```

```{r}
ggsave("plots/p_top_words_bipartisan_df.png", plot = p_top_words_bipartisan_df)
```

```{r}
top_words_bipartisan_df$normalized_count <- top_words_bipartisan_df$count / nrow(bipartisan_collection)

p_top_words_bipartisan_normalized <- ggplot(data=top_words_bipartisan_df, aes(x=reorder(word, -normalized_count), y=normalized_count)) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="(word count) / (number of tweets), bipartisan collection") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = -45, size =14),
    plot.title = element_text(hjust = 0.5)
  ) +
  ylim(0, 0.15)

p_top_words_bipartisan_normalized
```

```{r}
ggsave("plots/p_top_words_bipartisan_normalized.png", plot = p_top_words_bipartisan_normalized)
```

### Partisan collection

```{r}
top_words_partisan <- topfeatures(dtm_partisan_collection_a, 10)

top_words_partisan_df <- data.frame(count = top_words_partisan,
                               word = names(top_words_partisan))
row.names(top_words_partisan_df) <- NULL

p_top_words_partisan_df <- ggplot(data=top_words_partisan_df, aes(x=reorder(word, -count), y=count)) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="Top word count - Partisan Bill Tweets") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = -45, size =14),
    plot.title = element_text(hjust = 0.5)
  ) +
  ylim(0, 2000)

p_top_words_partisan_df
```

```{r}
ggsave("plots/p_top_words_partisan_df.png", plot = p_top_words_partisan_df)
```

```{r}
top_words_partisan_df$normalized_count <- top_words_partisan_df$count / nrow(partisan_collection)

p_top_words_partisan_normalized <- ggplot(data=top_words_partisan_df, aes(x=reorder(word, -normalized_count), y=normalized_count)) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="(word count) / (number of tweets), partisan collection") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = -45, size =14),
    plot.title = element_text(hjust = 0.5)
  ) +
  ylim(0, 0.15)

p_top_words_partisan_normalized
```

```{r}
ggsave("plots/p_top_words_partisan_normalized.png", plot = p_top_words_partisan_normalized)
```

The word "act" is ranked 2 in the partisan collection and 10 in the bipartisan collection.

Could be interesting to compare the frequencies. "american" has a higher percentage advantage over the 2nd most used word in the partisan collection than in the bipartisan.

## Top word pairings in compromise Tweets versus not compromise Tweets

```{r}
topfeatures(dtm_bipartisan_collection_b, 20)
```

```{r}
topfeatures(dtm_partisan_collection_b, 20)
```


## Top words in compromise bill Tweets that are only in compromise bill Tweets
Not sure how useful this will be. Try it and see. But may more be looking for words
that are rare in one and more common in another rather than frequent versus
completely absent. Maybe I get enough information about frequent words by
comparing top 20 words in compromise vs non compromise tweets

## Top words in non-compromise bills Tweets that are only in non-compromise bills Tweets

## Frequency of certain words in compromise versus non-compromise bill Tweets

These need to be compared against some baseline like number of Tweets or number of words/tokens.

https://stackoverflow.com/questions/65140308/in-r-how-can-i-count-specific-words-in-a-corpus

```{r}
dfm_target_words_bipartisan <- bipartisan_collection_corpus_a %>%
  tokens_select(c(
    "abortion",
    "agree",
    "agreement",
    "aisle",
    "bipartisan",
    "climate",
    "coalition",
    "collaboration",
    "collaborate",
    "compromise",
    "consensus",
    "conservative",
    "cooperation",
    "cooperate",
    "democrat",
    "family",
    "gun",
    "liberal",
    "moderate",
    "negotiation",
    "negotiate",
    "pass",
    "repeal",
    "republican",
    "solidarity",
    "today",
    "tax",
    "unite",
    "unity"
  )) %>%
  dfm()

freq_target_words_bipartisan <- textstat_frequency(dfm_target_words_bipartisan)
```

```{r}
freq_target_words_bipartisan$normalized_frequency <- freq_target_words_bipartisan$frequency / nrow(bipartisan_collection)

p_target_words_bipartisan_normalized <- ggplot(data=freq_target_words_bipartisan, aes(x=normalized_frequency, y=reorder(feature, normalized_frequency))) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="(word count) / (number of tweets), bipartisan collection") +
  theme_minimal() +
  theme(
    # axis.text.x = element_text(angle = 45, size =10),
    plot.title = element_text(hjust = 0.5)
  ) +
  xlim(0, 0.10)

p_target_words_bipartisan_normalized
```

```{r}
ggsave("plots/p_target_words_bipartisan_normalized.png", plot = p_target_words_bipartisan_normalized)
```



```{r}
dfm_target_words_partisan <- partisan_collection_corpus_a %>%
  tokens_select(c(
    "abortion",
    "agree",
    "agreement",
    "aisle",
    "bipartisan",
    "climate",
    "coalition",
    "collaboration",
    "collaborate",
    "compromise",
    "consensus",
    "conservative",
    "cooperation",
    "cooperate",
    "democrat",
    "family",
    "gun",
    "liberal",
    "moderate",
    "negotiation",
    "negotiate",
    "pass",
    "repeal",
    "republican",
    "solidarity",
    "today",
    "tax",
    "unite",
    "unity"
  )) %>%
  dfm()

freq_target_words_partisan <- textstat_frequency(dfm_target_words_partisan)
```

```{r}
freq_target_words_partisan$normalized_frequency <- freq_target_words_partisan$frequency / nrow(partisan_collection)

p_target_words_partisan_normalized <- ggplot(data=freq_target_words_partisan, aes(x=normalized_frequency, y=reorder(feature, normalized_frequency))) +
  geom_bar(stat="identity", fill="firebrick", width = 0.5)+
  labs(x = "", y="", title="(word count) / (number of tweets), partisan collection") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  ) +
  xlim(0, 0.10)

p_target_words_partisan_normalized
```

```{r}
ggsave("plots/p_target_words_partisan_normalized.png", plot = p_target_words_partisan_normalized)
```


# Statstical model 
## likelihood of certain words being in compromise bill Tweets

This should be useful:https://quanteda.io/articles/quickstart.html#grouping-words-by-dictionary-or-equivalence-class

Each Tweet will be viewed as a document. But I can add columns to each dataset
(before making the corpus). One column for `compromise` and one for `act` to 
distinguish whether this Tweet belongs to a compromise bill and what the name of
the bill is for each Tweet.

When do this, could combine all datasets into one to make one big corpus tagged
with the data I need for analysis.

Data prep
```{r}
## narrow dfm to several target features
dfm_target_words_all <- all_tweets_corpus_a %>%
  tokens_select(c(
    "abortion",
    "act",
    "agreement",
    "aisle",
    "american",
    "bipartisan",
    "climate",
    "coalition",
    "collaboration",
    "collaborate",
    "compromise",
    "consensus",
    "conservative",
    "cooperation",
    "cooperate",
    "democrat",
    "family",
    "gun",
    "liberal",
    "moderate",
    "negotiation",
    "negotiate",
    "pass",
    "repeal",
    "republican",
    "solidarity",
    "today",
    "tax",
    "unite",
    "unity"
  )) %>%
  dfm()

## convert to data frame
target_words_all <- convert(dfm_target_words_all, "data.frame")
```


```{r}
textstat_frequency(dfm_target_words_all)
```

```{r}
## add columns to Tweet collection based on dfm data
all_tweets$word_count_bipartisan = NA
all_tweets$word_present_bipartisan = NA

all_tweets$word_count_american = NA
all_tweets$word_count_act = NA
all_tweets$word_count_family = NA
all_tweets$word_count_today = NA
all_tweets$word_count_pass = NA
all_tweets$word_count_democrat = NA
all_tweets$word_count_republican = NA
all_tweets$word_count_climate = NA
all_tweets$word_count_tax = NA
all_tweets$word_count_gun = NA
all_tweets$word_count_unite = NA
all_tweets$word_count_abortion = NA
all_tweets$word_count_agreement = NA
all_tweets$word_count_conservative = NA
all_tweets$word_count_negotiate = NA
all_tweets$word_count_liberal = NA

for(i in 1:nrow(all_tweets)) {
  tweet_id <- all_tweets[i, "id_str"]
  
  current_target_words <- target_words_all[target_words_all$doc_id == tweet_id, ]# current row of target words
  
  bipartisan_count <- current_target_words$bipartisan
  american_count <- current_target_words$american
  act_count <- current_target_words$act
  family_count <- current_target_words$family
  today_count <- current_target_words$today
  pass_count <- current_target_words$pass
  democrat_count <- current_target_words$democrat
  republican_count <- current_target_words$republican
  climate_count <- current_target_words$climate
  tax_count <- current_target_words$tax
  gun_count <- current_target_words$gun
  unite_count <- current_target_words$unite
  abortion_count <- current_target_words$abortion
  agreement_count <- current_target_words$agreement
  conservative_count <- current_target_words$conservative
  negotiate_count <- current_target_words$negotiate
  liberal_count <- current_target_words$liberal

  
  all_tweets[i, "word_count_bipartisan"] <- bipartisan_count
  all_tweets[i, "word_count_american"] <- american_count
  all_tweets[i, "word_count_act"] <- act_count
  all_tweets[i, "word_count_family"] <- family_count
  all_tweets[i, "word_count_today"] <- today_count
  all_tweets[i, "word_count_pass"] <- pass_count
  all_tweets[i, "word_count_democrat"] <- democrat_count
  all_tweets[i, "word_count_republican"] <- republican_count
  all_tweets[i, "word_count_climate"] <- climate_count
  all_tweets[i, "word_count_tax"] <- tax_count
  all_tweets[i, "word_count_gun"] <- gun_count
  all_tweets[i, "word_count_unite"] <- unite_count
  all_tweets[i, "word_count_abortion"] <- abortion_count
  all_tweets[i, "word_count_agreement"] <- agreement_count
  all_tweets[i, "word_count_conservative"] <- conservative_count
  all_tweets[i, "word_count_negotiate"] <- negotiate_count
  all_tweets[i, "word_count_liberal"] <- liberal_count
  
  
  word_present_bipartisan = 0
  if(bipartisan_count > 0) {
    word_present_bipartisan = 1
  }
  all_tweets[i, "word_present_bipartisan"] <- word_present_bipartisan
}
```
Now we have two new columns in the all_tweets dataset.

The column word_count_bipartisan contains a count of the number of times the word "bipartisan" appears in the
Tweet.

The column word_present_bipartisan is coded as a 1 if the word bipartisan is present at least once
in the Tweet and 0 if it is not.

Other words to consider:
- mutual respect
- common ground
- bridge-building

Make categorical variables factors
```{r}
all_tweets$Race <- as.factor(all_tweets$Race)
all_tweets$gender <- as.factor(all_tweets$gender)
all_tweets$party <- as.factor(all_tweets$party)
all_tweets$bipartisan_bill <- as.factor(all_tweets$bipartisan_bill)
all_tweets$word_present_bipartisan <- as.factor(all_tweets$word_present_bipartisan)
```


## Exploratory analysis

```{r}
p_all_tweets_race <- ggplot(data = all_tweets) +
  geom_bar(mapping = aes(x = Race))
```

```{r}
ggsave("plots/p_all_tweets_race.png", plot = p_all_tweets_race)
```

Similar distribution of Senators vs Tweet count by race
```{r}
p_senator_variables_race <- ggplot(data = senator_variables) +
  geom_bar(mapping = aes(x = Race))
```

```{r}
ggsave("plots/p_senator_variables_race.png", plot = p_senator_variables_race)
```

```{r}
p_all_tweets_age <- ggplot(data = all_tweets) +
  geom_histogram(mapping = aes(x = age))

ggsave("plots/p_all_tweets_age.png", plot = p_all_tweets_age)
```

```{r}
p_all_tweets_gender <- ggplot(data = all_tweets) +
  geom_bar(mapping = aes(x = gender))

ggsave("plots/p_all_tweets_gender.png", plot = p_all_tweets_gender)
```

```{r}
p_all_tweets_bipartisan <- ggplot(data = all_tweets) +
  geom_bar(mapping = aes(x = bipartisan_bill)) +
  labs(x='') +
  scale_x_discrete(labels = c("0" = "Partisan bill", "1" = "Bipartisan bill"))

ggsave("plots/p_all_tweets_bipartisan.png", plot = p_all_tweets_bipartisan)
```

```{r}
p_all_tweets_bipartisan_votes <- ggplot(data = all_tweets) +
  geom_histogram(mapping = aes(x = bipartisan_votes), binwidth = 0.5)

ggsave("plots/p_all_tweets_bipartisan_votes.png", p_all_tweets_bipartisan_votes)
```

```{r}
# p_all_tweets_bipartisan_votes_gender <- ggplot(data = all_tweets) +
#   geom_count(mapping = aes(x = bipartisan_votes, y = gender))

p_all_tweets_bipartisan_votes_gender <- ggplot(data = all_tweets, mapping = aes(x = bipartisan_votes)) +
  geom_freqpoly(mapping = aes(colour = gender), binwidth = 1)

# p_all_tweets_bipartisan_votes_gender <- ggplot(data = all_tweets, aes(x = gender, y = bipartisan_votes)) +
#   geom_point() +
#   geom_jitter()

p_all_tweets_bipartisan_votes_gender

ggsave("plots/p_all_tweets_bipartisan_votes_gender.png", p_all_tweets_bipartisan_votes_gender)
```

```{r}
# p_all_tweets_bipartisan_votes_race <- ggplot(data = all_tweets, mapping = aes(x = bipartisan_votes)) +
#   geom_freqpoly(mapping = aes(colour = Race), binwidth = 1)

p_all_tweets_bipartisan_votes_race <- ggplot(data = all_tweets, aes(x = Race, y = bipartisan_votes)) +
  geom_count()

p_all_tweets_bipartisan_votes_race

ggsave("plots/p_all_tweets_bipartisan_votes_race.png", p_all_tweets_bipartisan_votes_race)
```

```{r}
# p_all_tweets_bipartisan_votes_age <- ggplot(data = all_tweets, aes(x = age, y = bipartisan_votes)) +
#   geom_point() +
#   geom_jitter(width = 0.5, height = 0.5)

p_all_tweets_bipartisan_votes_age <- ggplot(data = all_tweets, mapping = aes(x = as.factor(bipartisan_votes), y = age)) +
  geom_boxplot()

p_all_tweets_bipartisan_votes_age

ggsave("plots/p_all_tweets_bipartisan_votes_age.png", p_all_tweets_bipartisan_votes_age)
```

```{r}
ggplot(all_tweets, aes(bipartisan_votes, word_count_bipartisan)) + 
  geom_point() +
  geom_smooth(method='lm', formula= y~x)
```

```{r}

ggplot(all_tweets, aes(word_count_american, bipartisan_votes)) + 
  geom_point() +
  # geom_smooth(method='lm', formula= y~x)
  geom_line(aes(y=pred))
```
```


## The models

```{r}
logit_bipartisan <- glm(bipartisan_bill ~ word_present_bipartisan + party + gender + age + Race, data=all_tweets, family="binomial")
summary(logit_bipartisan)
## odds ratios and 95% CI
exp(cbind(OR = coef(logit_bipartisan), confint(logit_bipartisan))) # see https://stats.oarc.ucla.edu/r/dae/logit-regression/
```

```{r}
logit_bipartisan <- glm(bipartisan_bill ~ word_count_bipartisan + word_count_american + word_count_family + word_count_today + word_count_pass + word_count_republican + word_count_democrat + word_count_act + word_count_unite + word_count_agreement + party + gender + age + Race, data=all_tweets, family="binomial")
summary(logit_bipartisan)
## odds ratios and 95% CI
exp(cbind(OR = coef(logit_bipartisan), confint(logit_bipartisan))) # see https://stats.oarc.ucla.edu/r/dae/logit-regression/
```


```{r}
lm_bipartisan <- lm(bipartisan_votes ~ word_present_bipartisan + party + gender + age + Race, data=all_tweets)

summary(lm_bipartisan)
```

```{r}
lm_bipartisan <- lm(bipartisan_votes ~ word_count_bipartisan + word_count_american + word_count_act + word_count_family + word_count_today + word_count_pass + word_count_democrat + word_count_republican + word_count_climate + word_count_tax + word_count_gun + word_count_unite + word_count_agreement + word_count_conservative + word_count_negotiate + word_count_liberal + party + gender + age + Race, data=all_tweets)

summary(lm_bipartisan)
```



```{r}
# linear model with Race releveled (using all_tweets_2)

lm_bipartisan <- lm(bipartisan_votes ~ word_count_bipartisan + word_count_american + word_count_act + word_count_family + word_count_today + word_count_pass + word_count_democrat + word_count_republican + word_count_climate + word_count_tax + word_count_gun + word_count_unite + word_count_agreement + word_count_conservative + word_count_negotiate + word_count_liberal + party + gender + age + Race, data=all_tweets_2)

summary(lm_bipartisan)
```


## Visual ideas

### Word clouds

See https://quanteda.io/articles/quickstart.html

```{r}
textplot_wordcloud(
  dtm_bipartisan_collection_a,
  max_words = 40,
  rotation = 0.25,
  color = rev(RColorBrewer::brewer.pal(10, "RdBu"))
)
```

```{r}
textplot_wordcloud(
  dtm_partisan_collection_a, max_words=40,
  rotation = 0.25,
  color = rev(RColorBrewer::brewer.pal(10, "RdBu"))
)
```

### Figure showing top word for each day


# Sentiment

## Sentiment scores of all Tweets

## Sentiment scores of each collection

## Sentiment scores of compromise collection

## Sentiment scores of non-compromise collection

## Likelihood of positive sentiment being in the compromise bill collection

## Visual ideas

### Barchart with percent positive Tweets in each collection

### Barchart with percent positive Tweets for compromise collections vs non compromise

### Top emotions conveyed barchart

